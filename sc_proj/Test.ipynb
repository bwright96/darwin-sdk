{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from time import sleep\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from amb_sdk.sdk import DarwinSdk\n",
    "import datetime\n",
    "ts = '{:%Y%m%d%H%M%S}'.format(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are logged in!\n"
     ]
    }
   ],
   "source": [
    "# Login\n",
    "from login import username, password\n",
    "ds = DarwinSdk()\n",
    "ds.set_url('https://amb-demo-api.sparkcognition.com/v1/')\n",
    "status, msg = ds.auth_login_user(username, password)\n",
    "\n",
    "if not status:\n",
    "    print(msg)\n",
    "else:\n",
    "    print('You are logged in!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formatted_dataframe(xls_file, year):\n",
    "    df = pd.read_excel(xls_file)\n",
    "    df = df[['DZRATING', 'DZCAMPUS','DPETALLC','DA0AT' + str(year) + 'R', 'DA0912DR' + str(year) + 'R','DAGC4X' + str(year) + 'R','DAGC5X' + str(year - 1) + 'R','DA0GR' + str(year) + 'N','DA0CT' + str(year) + 'R','DA0CC' + str(year) + 'R','DA0CSA' + str(year) + 'R','DA0CAA' + str(year) + 'R','DPSTTOSA','DPSTEXPA','DPFRAALLT','DPFRAALLK','DPFRASTAP','DZRVLOCP','DPFRAFEDP','DPFEAINST','DPFEAINSK','DISTSIZE','COMMTYPE', 'PROPWLTH', 'TAXRATE']]\n",
    "    df = df.rename(index=str, columns = {\"DZRATING\":\"rating\", \"DZCAMPUS\":\"num_schools\", \"DPETALLC\":\"num_students\", \"DA0AT\" + str(year) + \"R\":\"attendance_rate\", \"DA0912DR\" + str(year) + \"R\":\"dropout_rate\", \"DAGC4X\" + str(year) + \"R\":\"grad_rate_4_year\", \"DAGC5X\" + str(year - 1) + \"R\":\"grad_rate_5_year\", \"DA0GR\" + str(year) + \"N\":\"annual_grad_count\", \"DA0CT\" + str(year) + \"R\":\"college_admissions_per_tested\", \"DA0CC\" + str(year) + \"R\":\"college_admissions_at_crit\", \"DA0CSA\" + str(year) + \"R\":\"average_sat\", \"DA0CAA\" + str(year) + \"R\":\"average_act\", \"DPSTTOSA\":\"average_teacher_salary\", \"DPSTEXPA\":\"average_teacher_exp\", \"DPFRAALLT\":\"total_revenue\", \"DPFRAALLK\":\"total_revenue_per_pupil\", \"DPFRASTAP\":\"percent_revenue_from_state\", \"DZRVLOCP\":\"percent_revenue_from_local\", \"DPFRAFEDP\":\"percent_revenue_from_federal\", \"DPFEAINST\":\"instr_expenditures\", \"DPFEAINSK\":\"instr_expenditures_per_pupil\", \"DISTSIZE\":\"district_size\", \"COMMTYPE\":\"community_type\", \"PROPWLTH\":\"property_wealth\", \"TAXRATE\":\"tax_rate\"})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-a38deb5de4c2>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-a38deb5de4c2>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    train_df = get_formatted_dataframe('2015-2016.xls',15))\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "test_df = get_formatted_dataframe('2016-2017.xls', 16)\n",
    "train_df = get_formatted_dataframe('2015-2016.xls',15)\n",
    "train_df = train_df.append(get_formatted_dataframe('2014-2015.xls',14))\n",
    "train_df = train_df.append(get_formatted_dataframe('2013-2014.xls',13))\n",
    "print('Test dataframe shape:', test_df.shape)\n",
    "print('Train dataframe shape:', train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0:len(df)//5].to_csv('test_data.csv')\n",
    "df[len(df)//5:].to_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET='train_data.csv'\n",
    "TEST_DATASET = 'test_data.csv'\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "status, message = ds.delete_dataset(TRAIN_DATASET)\n",
    "if not status:\n",
    "    print(dataset)\n",
    "status, dataset = ds.upload_dataset( TRAIN_DATASET)\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean train_Set\n",
    "target = \"rating\"\n",
    "status, job_id = ds.clean_data(TRAIN_DATASET, target = target)\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = target + \"_model01\" + ts\n",
    "status, job_id = ds.create_model(dataset_names = TRAIN_DATASET, \\\n",
    "                                 model_name =  model, \\\n",
    "                                 max_train_time = '00:05')\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature importance of built model\n",
    "status, artifact = ds.analyze_model(model)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(TRAIN_DATASET, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(df[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(classification_report(df[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, job_id = ds.clean_data(TEST_DATASET, target = target, model_name = model)\n",
    "\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(TEST_DATASET, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots comparing predictions with actual target\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "df = pd.read_csv(TEST_DATASET)\n",
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(df[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(classification_report(df[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST DATASET\n",
    "status, dataset = ds.upload_dataset( TEST_DATASET)\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
