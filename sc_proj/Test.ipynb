{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from time import sleep\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from amb_sdk.sdk import DarwinSdk\n",
    "import datetime\n",
    "ts = '{:%Y%m%d%H%M%S}'.format(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are logged in!\n"
     ]
    }
   ],
   "source": [
    "# Login\n",
    "ds = DarwinSdk()\n",
    "ds.set_url('https://amb-demo-api.sparkcognition.com/v1/')\n",
    "status, msg = ds.auth_login_user('brandonwright@utexas.edu', 'wLXEQ8ffrB')\n",
    "\n",
    "if not status:\n",
    "    print(msg)\n",
    "else:\n",
    "    print('You are logged in!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_formatted_dataframe(xls_file, year):\n",
    "    df = pd.read_excel(xls_file)\n",
    "    df = df[['DZRATING', 'DZCAMPUS','DPETALLC','DA0AT' + str(year) + 'R', 'DA0912DR' + str(year) + 'R','DAGC4X' + str(year) + 'R','DAGC5X' + str(year - 1) + 'R','DA0GR' + str(year) + 'N','DA0CT' + str(year) + 'R','DA0CC' + str(year) + 'R','DA0CSA' + str(year) + 'R','DA0CAA' + str(year) + 'R','DPSTTOSA','DPSTEXPA','DPFRAALLT','DPFRAALLK','DPFRASTAP','DZRVLOCP','DPFRAFEDP','DPFEAINST','DPFEAINSK','DISTSIZE','COMMTYPE', 'PROPWLTH', 'TAXRATE']]\n",
    "    df = df.rename(index=str, columns = {\"DZRATING\":\"rating\", \"DZCAMPUS\":\"num_schools\", \"DPETALLC\":\"num_students\", \"DA0AT\" + str(year) + \"R\":\"attendance_rate\", \"DA0912DR\" + str(year) + \"R\":\"dropout_rate\", \"DAGC4X\" + str(year) + \"R\":\"grad_rate_4_year\", \"DAGC5X\" + str(year - 1) + \"R\":\"grad_rate_5_year\", \"DA0GR\" + str(year) + \"N\":\"annual_grad_count\", \"DA0CT\" + str(year) + \"R\":\"college_admissions_per_tested\", \"DA0CC\" + str(year) + \"R\":\"college_admissions_at_crit\", \"DA0CSA\" + str(year) + \"R\":\"average_sat\", \"DA0CAA\" + str(year) + \"R\":\"average_act\", \"DPSTTOSA\":\"average_teacher_salary\", \"DPSTEXPA\":\"average_teacher_exp\", \"DPFRAALLT\":\"total_revenue\", \"DPFRAALLK\":\"total_revenue_per_pupil\", \"DPFRASTAP\":\"percent_revenue_from_state\", \"DZRVLOCP\":\"percent_revenue_from_local\", \"DPFRAFEDP\":\"percent_revenue_from_federal\", \"DPFEAINST\":\"instr_expenditures\", \"DPFEAINSK\":\"instr_expenditures_per_pupil\", \"DISTSIZE\":\"district_size\", \"COMMTYPE\":\"community_type\", \"PROPWLTH\":\"property_wealth\", \"TAXRATE\":\"tax_rate\"})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataframe shape: (1203, 25)\n",
      "Train dataframe shape: (3653, 25)\n"
     ]
    }
   ],
   "source": [
    "test_df = get_formatted_dataframe('2016-2017.xls', 16)\n",
    "train_df = get_formatted_dataframe('2015-2016.xls',15)\n",
    "train_df = train_df.append(get_formatted_dataframe('2014-2015.xls',14))\n",
    "train_df = train_df.append(get_formatted_dataframe('2013-2014.xls',13))\n",
    "print('Test dataframe shape:', test_df.shape)\n",
    "print('Train dataframe shape:', train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataframe(df):\n",
    "    df = df.loc[(df['rating'] == \"Met Standard\") | (df['rating'] == \"Met Alternative Standard\") |(df['rating'] == \"Improvement Required\")]\n",
    "    features = list(df)[3:21]\n",
    "    for feature in features:\n",
    "        df = df.loc[(df[feature] != '.')]\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning:\n",
      "Test dataframe shape: (908, 25)\n",
      "Train dataframe shape: (2749, 25)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the data\n",
    "train_df = clean_dataframe(train_df)\n",
    "test_df = clean_dataframe(test_df)\n",
    "print('After cleaning:')\n",
    "print('Test dataframe shape:', test_df.shape)\n",
    "print('Train dataframe shape:', train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversampling:\n",
      "Met Standard                2589\n",
      "Improvement Required         107\n",
      "Met Alternative Standard      53\n",
      "Name: rating, dtype: int64\n",
      "\n",
      "After oversampling:\n",
      "Met Alternative Standard    2589\n",
      "Met Standard                2589\n",
      "Improvement Required        2589\n",
      "Name: rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Dealing with class imbalance\n",
    "\n",
    "print(\"Before oversampling:\")\n",
    "print(train_df['rating'].value_counts())\n",
    "print()\n",
    "\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "ad = SMOTENC(categorical_features=[20, 21, 22, 23], random_state=None)\n",
    "train_df_y = train_df['rating']\n",
    "train_df_x = train_df.iloc[:,1:]\n",
    "x_res, y_res = ad.fit_sample(train_df_x, train_df_y)\n",
    "\n",
    "train_df = pd.DataFrame(data=x_res)\n",
    "train_df['rating'] = y_res\n",
    "\n",
    "print(\"After oversampling:\")\n",
    "print(train_df['rating'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('test_data.csv')\n",
    "train_df.to_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET='train_data.csv'\n",
    "TEST_DATASET = 'test_data.csv'\n",
    "train_df = pd.read_csv('train_data.csv')\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "status, message = ds.delete_dataset(TRAIN_DATASET)\n",
    "status, dataset = ds.upload_dataset( TRAIN_DATASET)\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 'Requested', 'starttime': '2019-04-22T00:14:53.625938', 'endtime': None, 'percent_complete': 0, 'job_type': 'CleanDataTiny', 'loss': None, 'generations': None, 'dataset_names': ['train_data.csv'], 'artifact_names': ['2e6729ef06894636a6ff4c0150752bd5'], 'model_name': None, 'job_error': None}\n",
      "{'status': 'Complete', 'starttime': '2019-04-22T00:14:53.625938', 'endtime': '2019-04-22T00:14:57.026071', 'percent_complete': 100, 'job_type': 'CleanDataTiny', 'loss': None, 'generations': None, 'dataset_names': ['train_data.csv'], 'artifact_names': ['2e6729ef06894636a6ff4c0150752bd5'], 'model_name': None, 'job_error': ''}\n"
     ]
    }
   ],
   "source": [
    "#clean train_Set\n",
    "target = \"rating\"\n",
    "status, job_id = ds.clean_data(TRAIN_DATASET, target = target)\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature importance of built model\n",
    "status, artifact = ds.analyze_model(model)\n",
    "sleep(1)\n",
    "if status:\n",
    "    ds.wait_for_job(artifact['job_name'])\n",
    "else:\n",
    "    print(artifact)\n",
    "status, feature_importance = ds.download_artifact(artifact['artifact_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_importance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-71d9fe97782b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_importance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_importance' is not defined"
     ]
    }
   ],
   "source": [
    "feature_importance[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(UPSAMPLE_TRAIN_DATASET, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(upsample_train_df[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(classification_report(upsample_train_df[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TEST DATASET\n",
    "status, message = ds.delete_dataset(TEST_DATASET)\n",
    "status, dataset = ds.upload_dataset( TEST_DATASET)\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, job_id = ds.clean_data(TEST_DATASET, target = target)\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds.download_dataset(job_id['artifact_name'], artifact_path='/Users/ctjoe/Documents/CS363D/darwin/darwin-sdk/sc_proj')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_test_df = pd.read_csv(dataset[1]['filename'])\n",
    "print(clean_test_df.head())\n",
    "clean_test_df['rating'] = 'N/A'\n",
    "clean_test_df.loc[clean_test_df['rating$$Met Standard'] == 1.0, 'rating'] = 1.0\n",
    "clean_test_df.loc[clean_test_df['rating$$Met Alternative Standard'] == 1.0, 'rating'] = 2.0\n",
    "clean_test_df.loc[clean_test_df['rating$$Improvement Required'] == 1.0, 'rating'] = 0\n",
    "labels = list(clean_test_df)\n",
    "labels.remove('rating$$Improvement Required')\n",
    "labels.remove('rating$$Met Alternative Standard')\n",
    "labels.remove('rating$$Met Standard')\n",
    "clean_test_df = clean_test_df[labels]\n",
    "labels.remove('rating')\n",
    "clean_test_df.loc[clean_test_df['rating'] == 1.0, 'rating'] = 'Met Standard'\n",
    "clean_test_df.loc[clean_test_df['rating'] == 2.0, 'rating'] = 'Met Alternative Standard'\n",
    "clean_test_df.loc[clean_test_df['rating'] == 0.0, 'rating'] = 'Improvement Required'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_test_df.to_csv('modified_test_data.csv')\n",
    "MODIFIED_TEST_DATASET = 'modified_test_data.csv'\n",
    "modified_test_df = pd.read_csv('modified_test_data.csv')\n",
    "status, message = ds.delete_dataset(MODIFIED_TEST_DATASET)\n",
    "status, dataset = ds.upload_dataset(MODIFIED_TEST_DATASET)\n",
    "if not status:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "status, job_id = ds.clean_data(MODIFIED_TEST_DATASET, target = target, model_name = model)\n",
    "if status:\n",
    "    ds.wait_for_job(job_id['job_name'])\n",
    "else:\n",
    "    print(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status, artifact = ds.run_model(MODIFIED_TEST_DATASET, model)\n",
    "sleep(1)\n",
    "ds.wait_for_job(artifact['job_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots comparing predictions with actual target\n",
    "status, prediction = ds.download_artifact(artifact['artifact_name'])\n",
    "df = pd.read_csv(MODIFIED_TEST_DATASET)\n",
    "unq = prediction[target].unique()[::-1]\n",
    "p = np.zeros((len(prediction),))\n",
    "a = np.zeros((len(prediction),))\n",
    "for i,q in enumerate(unq):\n",
    "    p += i*(prediction[target] == q).values\n",
    "    a += i*(df[target] == q).values\n",
    "#Plot predictions vs actual\n",
    "plt.plot(a)\n",
    "plt.plot(p)\n",
    "plt.legend(['Actual','Predicted'])\n",
    "plt.yticks([i for i in range(len(unq))],[q for q in unq]);\n",
    "print(classification_report(df[target], prediction[target]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
